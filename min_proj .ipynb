{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcd03eb",
   "metadata": {},
   "source": [
    "# music system based on facial expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1d1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rawat\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\rawat\\AppData\\Local\\Temp/ipykernel_1800/99326359.py:49: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.8286 - accuracy: 0.2428 - val_loss: 1.8338 - val_accuracy: 0.2500\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.7932 - accuracy: 0.2616 - val_loss: 1.7856 - val_accuracy: 0.2719\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 205s 1s/step - loss: 1.6993 - accuracy: 0.3266 - val_loss: 1.7028 - val_accuracy: 0.3719\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 139s 692ms/step - loss: 1.6098 - accuracy: 0.3770 - val_loss: 1.4891 - val_accuracy: 0.4531\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 123s 614ms/step - loss: 1.5587 - accuracy: 0.4011 - val_loss: 1.5010 - val_accuracy: 0.4500\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize image data generator with rescaling\n",
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all training images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Preprocess all validation images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "# Train the neural network/model\n",
    "emotion_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=200,  # Adjust the number of steps per epoch as needed\n",
    "    epochs=5,  # Adjust the number of epochs as needed\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=5  # Adjust the number of validation steps as needed\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "emotion_model.save('emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ef6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Final Emotion: Sad\n",
      "Now playing: sad1.mp3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Load the pre-trained CNN model for facial expression recognition\n",
    "emotion_model = load_model('emotion_model.h5')\n",
    "\n",
    "# Define the list of emotion labels\n",
    "emotion_labels = ['Angry', 'Disgusted', 'Fearful', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
    "\n",
    "# Start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow('Emotion Detection')\n",
    "\n",
    "# Variables to store emotion predictions\n",
    "emotion_predictions = []\n",
    "\n",
    "# Set the duration for capturing frames (in seconds)\n",
    "duration = 10\n",
    "\n",
    "# Set the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Mood-based song recommendations\n",
    "mood_songs = {\n",
    "    \"happy\": [\"Luke-Bergs-Bliss.mp3\",\"Feel-Good.mp3\"],\n",
    "    \"sad\": [\"sad1.mp3\",\"sad2.mp3\"],\n",
    "    \"Neutral\": [\"neutral1.mp3\",]\n",
    "    # Add more mood-song mappings as needed\n",
    "}\n",
    "\n",
    "# Set the prediction frequency (process every nth frame)\n",
    "prediction_frequency = 5\n",
    "frame_counter = 0\n",
    "\n",
    "# Flag to indicate if the song is currently playing\n",
    "song_playing = False\n",
    "\n",
    "while time.time() - start_time < duration:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_counter += 1\n",
    "\n",
    "    # Process every nth frame\n",
    "    if frame_counter % prediction_frequency == 0:\n",
    "        # Preprocess the frame\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.resize(gray, (48, 48))\n",
    "        gray = np.reshape(gray, (1, 48, 48, 1))\n",
    "        gray = gray.astype('float32') / 255\n",
    "\n",
    "        # Make a prediction on the preprocessed frame\n",
    "        emotion_prediction = emotion_model.predict(gray)\n",
    "        emotion_index = np.argmax(emotion_prediction)\n",
    "        emotion_label = emotion_labels[emotion_index]\n",
    "        \n",
    "        # Append the predicted emotion to the list\n",
    "        emotion_predictions.append(emotion_label)\n",
    "\n",
    "        # Display the emotion label on the frame\n",
    "        cv2.putText(frame, emotion_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "     \n",
    "    # Display the frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "    # Check if the 's' key is pressed to stop the song\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s') and song_playing:\n",
    "        # Stop the song using pygame mixer\n",
    "        pygame.mixer.music.stop()\n",
    "        song_playing = False\n",
    "        print(\"Song stopped.\")\n",
    "        # Clear the output to remove the previous displayed frame\n",
    "        clear_output(wait=True)\n",
    "        display(frame)\n",
    "        \n",
    "    # Check if the 's' key is pressed to save the frame\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite('saved_frame.jpg', frame)\n",
    "        print(\"Frame saved as saved_frame.jpg.\")\n",
    "        \n",
    "# Determine the final emotion based on the majority vote\n",
    "final_emotion = max(set(emotion_predictions), key=emotion_predictions.count)\n",
    "\n",
    "# Print the final emotion\n",
    "print(\"Final Emotion:\", final_emotion)\n",
    "\n",
    "# Check if songs are available for the final emotion\n",
    "songs = mood_songs.get(final_emotion.lower())\n",
    "if songs:\n",
    "    # Select a random song from the available songs\n",
    "    song = random.choice(songs)\n",
    "    \n",
    "    # Load and play the song using pygame mixer\n",
    "    pygame.mixer.music.load(song)\n",
    "    pygame.mixer.music.play()\n",
    "    song_playing = True\n",
    "    print(f\"Now playing: {song}\")\n",
    "else:\n",
    "    print(\"Sorry, no songs found for the specified mood.\")\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d047f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a4480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447df222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
